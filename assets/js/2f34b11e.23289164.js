"use strict";(self.webpackChunkgrid_docs=self.webpackChunkgrid_docs||[]).push([[8393],{3905:function(e,n,t){t.d(n,{Zo:function(){return g},kt:function(){return u}});var r=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function o(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=r.createContext({}),c=function(e){var n=r.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},g=function(e){var n=c(e.components);return r.createElement(l.Provider,{value:n},e.children)},p={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,g=o(e,["components","mdxType","originalType","parentName"]),d=c(t),u=a,m=d["".concat(l,".").concat(u)]||d[u]||p[u]||i;return t?r.createElement(m,s(s({ref:n},g),{},{components:t})):r.createElement(m,s({ref:n},g))}));function u(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,s=new Array(i);s[0]=d;var o={};for(var l in n)hasOwnProperty.call(n,l)&&(o[l]=n[l]);o.originalType=e,o.mdxType="string"==typeof e?e:a,s[1]=o;for(var c=2;c<i;c++)s[c]=t[c];return r.createElement.apply(null,s)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},1907:function(e,n,t){t.r(n),t.d(n,{frontMatter:function(){return o},contentTitle:function(){return l},metadata:function(){return c},toc:function(){return g},default:function(){return d}});var r=t(7462),a=t(3366),i=(t(7294),t(3905)),s=["components"],o={description:"Training Generative Adversarial Network using PyTorch Lightning"},l="GANs using PyTorch Lightning",c={unversionedId:"examples/vision/gans-using-pytorch-lightning",id:"examples/vision/gans-using-pytorch-lightning",title:"GANs using PyTorch Lightning",description:"Training Generative Adversarial Network using PyTorch Lightning",source:"@site/docs/examples/vision/gans-using-pytorch-lightning.md",sourceDirName:"examples/vision",slug:"/examples/vision/gans-using-pytorch-lightning",permalink:"/examples/vision/gans-using-pytorch-lightning",editUrl:"https://github.com/gridai/grid-docs/edit/master/docs/examples/vision/gans-using-pytorch-lightning.md",tags:[],version:"current",lastUpdatedAt:1645053639,formattedLastUpdatedAt:"2/16/2022",frontMatter:{description:"Training Generative Adversarial Network using PyTorch Lightning"}},g=[{value:"Goal",id:"goal",children:[],level:2},{value:"What are GANs?",id:"what-are-gans",children:[],level:2},{value:"The model",id:"the-model",children:[],level:2},{value:"Training",id:"training",children:[],level:2},{value:"Visualizing results",id:"visualizing-results",children:[],level:2}],p={toc:g};function d(e){var n=e.components,o=(0,a.Z)(e,s);return(0,i.kt)("wrapper",(0,r.Z)({},p,o,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"gans-using-pytorch-lightning"},"GANs using PyTorch Lightning"),(0,i.kt)("h2",{id:"goal"},"Goal"),(0,i.kt)("p",null,"This example covers how to train a Generative Adversarial Network ","(","GAN",")"," using Pytorch Lightning; the dataset used is cifar-10 images"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"What are GANs"),(0,i.kt)("li",{parentName:"ol"},"The model"),(0,i.kt)("li",{parentName:"ol"},"Training"),(0,i.kt)("li",{parentName:"ol"},"Visualizing results")),(0,i.kt)("h2",{id:"what-are-gans"},"What are GANs?"),(0,i.kt)("p",null,"Generative Adversarial Networks, or GANs are an approach to generative modeling using deep learning methods, such as convolutional neural networks. ",(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1406.2661"},"https://arxiv.org/abs/1406.2661")),(0,i.kt)("p",null,"In this approach, the generative model is pitted against an adversary or discriminative model that learns to determine whether a sample image is from the model distribution or the data distribution."),(0,i.kt)("h2",{id:"the-model"},"The model"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/PyTorchLightning/lightning-bolts"},"PyTorch Lightning Bolts")," contains many state of the art pre-trained model recipes. For this example we will use the basic gan script to train a basic generative adversarial network; check out the code ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/PyTorchLightning/lightning-bolts/blob/master/pl_bolts/models/gans/basic/basic_gan_module.py"},"here"),". The dataset argument can train for any standard image dataset; in this example we choose cifar10"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-text"},'from argparse import ArgumentParser\n\nimport torch\nfrom pytorch_lightning import LightningModule, seed_everything, Trainer\nfrom torch.nn import functional as F\n\nfrom pl_bolts.models.gans.basic.components import Discriminator, Generator\n\n\nclass GAN(LightningModule):\n    """\n    Vanilla GAN implementation.\n    Example::\n        from pl_bolts.models.gans import GAN\n        m = GAN()\n        Trainer(gpus=2).fit(m)\n    Example CLI::\n        # mnist\n        python  basic_gan_module.py --gpus 1\n        # imagenet\n        python  basic_gan_module.py --gpus 1 --dataset \'imagenet2012\'\n        --data_dir /path/to/imagenet/folder/ --meta_dir ~/path/to/meta/bin/folder\n        --batch_size 256 --learning_rate 0.0001\n    """\n')),(0,i.kt)("h2",{id:"training"},"Training"),(0,i.kt)("p",null,"Training this model using Grid, we are going to use the Web application. Login to Grid and start new Run; in the Github repo box; paste the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/master/pl_bolts/models/gans/basic/basic_gan_module.py"},"script"),". Choose any CPU or GPU and lightning framework. Learning rate and dataset can be specified as a script argument."),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(577).Z})),(0,i.kt)("p",null,"For this example, we used these values"),(0,i.kt)("p",null,"--learning","_",'rate "uniform',"(","1e-5, 1e-1, 3",")",'" and --dataset "',"[","'cifar10'","]",'"'),(0,i.kt)("p",null,"Next, see the model training, generating metrics and artifacts; download as necessary"),(0,i.kt)("h2",{id:"visualizing-results"},"Visualizing results"),(0,i.kt)("p",null,"The basic gan model recipe generates metrics such as loss which can be visualized in the web interface; here you see the generator loss and discriminator loss;"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-text"},"def generator_loss(self, x):\n        # sample noise\n        z = torch.randn(x.shape[0], self.hparams.latent_dim, device=self.device)\n        y = torch.ones(x.size(0), 1, device=self.device)\n\n        # generate images\n        generated_imgs = self(z)\n\n        D_output = self.discriminator(generated_imgs)\n\n        # ground truth result (ie: all real)\n        g_loss = F.binary_cross_entropy(D_output, y)\n\n        return g_loss\n\n    def discriminator_loss(self, x):\n        # train discriminator on real\n        b = x.size(0)\n        x_real = x.view(b, -1)\n        y_real = torch.ones(b, 1, device=self.device)\n\n        # calculate real score\n        D_output = self.discriminator(x_real)\n        D_real_loss = F.binary_cross_entropy(D_output, y_real)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(2251).Z})),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"This model will train for a long time and is just an example; stop experiments at any time"))))}d.isMDXComponent=!0},2251:function(e,n,t){n.Z=t.p+"assets/images/gans-metrics-f230d8af80a3b48a29fa26075fabae18.png"},577:function(e,n,t){n.Z=t.p+"assets/images/cifar-10-script-arguments-82a81fbe160414e9ffc5ce2411f05abf.gif"}}]);
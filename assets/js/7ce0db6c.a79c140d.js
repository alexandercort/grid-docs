"use strict";(self.webpackChunkgrid_docs=self.webpackChunkgrid_docs||[]).push([[4098],{3905:function(e,t,i){i.d(t,{Zo:function(){return d},kt:function(){return m}});var n=i(7294);function a(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function r(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,n)}return i}function o(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?r(Object(i),!0).forEach((function(t){a(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):r(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function s(e,t){if(null==e)return{};var i,n,a=function(e,t){if(null==e)return{};var i,n,a={},r=Object.keys(e);for(n=0;n<r.length;n++)i=r[n],t.indexOf(i)>=0||(a[i]=e[i]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)i=r[n],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(a[i]=e[i])}return a}var l=n.createContext({}),c=function(e){var t=n.useContext(l),i=t;return e&&(i="function"==typeof e?e(t):o(o({},t),e)),i},d=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var i=e.components,a=e.mdxType,r=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=c(i),m=a,f=u["".concat(l,".").concat(m)]||u[m]||p[m]||r;return i?n.createElement(f,o(o({ref:t},d),{},{components:i})):n.createElement(f,o({ref:t},d))}));function m(e,t){var i=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=i.length,o=new Array(r);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,o[1]=s;for(var c=2;c<r;c++)o[c]=i[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,i)}u.displayName="MDXCreateElement"},1671:function(e,t,i){i.r(t),i.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return c},toc:function(){return d},default:function(){return u}});var n=i(7462),a=i(3366),r=(i(7294),i(3905)),o=["components"],s={description:"Tutorial shows how to train Video Classification Models on Grid using Lightning Flash"},l="Kinetics Video Classification",c={unversionedId:"examples/vision/kinetics-video-classification",id:"examples/vision/kinetics-video-classification",title:"Kinetics Video Classification",description:"Tutorial shows how to train Video Classification Models on Grid using Lightning Flash",source:"@site/docs/examples/vision/kinetics-video-classification.md",sourceDirName:"examples/vision",slug:"/examples/vision/kinetics-video-classification",permalink:"/examples/vision/kinetics-video-classification",editUrl:"https://github.com/gridai/grid-docs/edit/master/docs/examples/vision/kinetics-video-classification.md",tags:[],version:"current",lastUpdatedAt:1644927410,formattedLastUpdatedAt:"2/15/2022",frontMatter:{description:"Tutorial shows how to train Video Classification Models on Grid using Lightning Flash"}},d=[{value:"Goal",id:"goal",children:[],level:2},{value:"Tutorial time",id:"tutorial-time",children:[],level:2},{value:"Task: Video Classification",id:"task-video-classification",children:[],level:2},{value:"Dataset: Kinetics",id:"dataset-kinetics",children:[],level:2},{value:"Step 1: Model",id:"step-1-model",children:[],level:2},{value:"Step 2: Start a RUN",id:"step-2-start-a-run",children:[],level:2},{value:"Step 3: Use the model for predictions",id:"step-3-use-the-model-for-predictions",children:[],level:2},{value:"Bonus: CLI equivalent",id:"bonus-cli-equivalent",children:[],level:2}],p={toc:d};function u(e){var t=e.components,s=(0,a.Z)(e,o);return(0,r.kt)("wrapper",(0,n.Z)({},p,s,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"kinetics-video-classification"},"Kinetics Video Classification"),(0,r.kt)("h2",{id:"goal"},"Goal"),(0,r.kt)("p",null,"This example covers a video classification deep learning task"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"What is Video Classification"),(0,r.kt)("li",{parentName:"ol"},"Training the model using ",(0,r.kt)("inlineCode",{parentName:"li"},"train.py")," script"),(0,r.kt)("li",{parentName:"ol"},"Loading Grid Weights in Flash and Running Object Detector")),(0,r.kt)("h2",{id:"tutorial-time"},"Tutorial time"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"5 minutes")),(0,r.kt)("h2",{id:"task-video-classification"},"Task: Video Classification"),(0,r.kt)("p",null,"Video Classification is the task of classifying videos based on classes of content. Video Classification enables computers to recognize actions, objects, and events in videos. From Retail, Health Care to Agriculture, Video Understanding enables automation of ",(0,r.kt)("a",{parentName:"p",href:"https://www.cio.com/article/3431138/ai-gets-the-picture-streamlining-business-processes-with-image-and-video-classification.html"},"countless industry use cases"),"."),(0,r.kt)("h2",{id:"dataset-kinetics"},"Dataset: Kinetics"),(0,r.kt)("p",null,"The Kinetics Human Action Video Dataset released by DeepMind ","(",(0,r.kt)("a",{parentName:"p",href:"https://deepmind.com/research/open-source/kinetics"},"reference"),")"," is a large-scale video understanding dataset comprised of annotated~10s video clips sourced from YouTube."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Example Kinetics Video Thumbnails",src:i(6816).Z})),(0,r.kt)("h2",{id:"step-1-model"},"Step 1: Model"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://lightning-flash.readthedocs.io/en/latest/reference/object_detection.html"},"PyTorch Lightning Flash")," enables quick training, fine tuning, and inferencing of SOTA video classification."),(0,r.kt)("p",null,"For this demo, we're going to be using the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/aribornstein/KineticsDemo"},"code here")),(0,r.kt)("h2",{id:"step-2-start-a-run"},"Step 2: Start a RUN"),(0,r.kt)("p",null,"Training this model on Grid has 4 simple steps or you can click ",(0,r.kt)("a",{parentName:"p",href:"https://platform.grid.ai/#/runs?script=https://github.com/aribornstein/KineticsDemo/blob/4fcf30e1c2fd46247ec0fc1a6cb0886e9838586f/train.py&cloud=grid&instance=g4dn.xlarge&accelerators=1&disk_size=200&framework=lightning&script_args=--grid_name%20transformers-run%20%5C%0A--grid_strategy%20grid_search%20%5C%0A--grid_disk_size%20200%20%5C%0A--grid_max_nodes%2010%20%5C%0A--grid_datastore_mount_dir%20%2Fopt%2Fdatastore%20%5C%0A--grid_instance_type%20p3.2xlarge%20%5C%0A--grid_credential%20cc-b87v8%20%5C%0A--grid_framework%20lightning%20%5C%0A--grid_gpus%201%20%5C%0Atrain.py%20--gpus%201%20--max_epochs%2010"},"here")," to get started:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Create a ",(0,r.kt)("strong",{parentName:"li"},"Run"),"."),(0,r.kt)("li",{parentName:"ul"},"Copy and paste the model script.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"https://github.com/aribornstein/KineticsDemo/blob/main/train.py\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Select  1xT4 ","(","16 GB",")"," $0.68/h ","(","g4dn.xlarge",")"," as the Accelerator"),(0,r.kt)("li",{parentName:"ul"},"Provide the  run arguments ",(0,r.kt)("inlineCode",{parentName:"li"},"--max_epochs 5")," ",(0,r.kt)("inlineCode",{parentName:"li"},"--gpus 1"))),(0,r.kt)("p",null,"You can add optional flags to this script:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"--train_folder /path/to/train_videos\n--val_folder /path/to/val_videos\n--download False\n")),(0,r.kt)("h2",{id:"step-3-use-the-model-for-predictions"},"Step 3: Use the model for predictions"),(0,r.kt)("p",null,"In this step, we load the Grid weights in Flash and run the model to classify videos."),(0,r.kt)("p",null,"1.Download Artifacts from Grid Run"),(0,r.kt)("p",null,(0,r.kt)("img",{src:i(3918).Z})),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Load model to our script and inference in 4 lines of code.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from flash.core.utilities.imports import _KORNIA_AVAILABLE, _PYTORCHVIDEO_AVAILABLE\nfrom flash.video import VideoClassifier\n\n# 1. Load the model\nmodel = VideoClassifier.load_from_checkpoint("video_classification_model.pts")\n\n# 2. Make a prediction\npredictions = model.predict("path/to/video.png")\nprint(predictions)\n')),(0,r.kt)("p",null,"Congratulations you have successfully trained and run inference for your first Video Classification Model with Grid."),(0,r.kt)("h2",{id:"bonus-cli-equivalent"},"Bonus: CLI equivalent"),(0,r.kt)("p",null,"Here are the equivalent commands for the CLI"),(0,r.kt)("p",null,"First, clone the project"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"git clone https://github.com/aribornstein/KineticsDemo.git\ncd KineticsDemo\n")),(0,r.kt)("p",null,"Start run"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"grid run \\\n--name $1 \\\n--disk_size 200 \\\n--instance_type g4dn.xlarge \\\n--gpus 1 \\\ntrain.py \\\n--gpus 1\n--fast_dev_run 1\n")))}u.isMDXComponent=!0},3918:function(e,t,i){t.Z=i.p+"assets/images/kinetics-artifacts-d8046d39dbfd29c6585e6364a8403cf9.png"},6816:function(e,t,i){t.Z=i.p+"assets/images/kinetics-images-958b126b2cce3156ff78c1d2162ba28e.png"}}]);